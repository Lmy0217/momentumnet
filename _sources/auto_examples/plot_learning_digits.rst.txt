
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_learning_digits.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_learning_digits.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_learning_digits.py:


=========================================
Momentum ResNets on a digit learning task
=========================================

In this example we train a Momentum ResNet and a ResNet
on the sklearn digit dataset

.. GENERATED FROM PYTHON SOURCE LINES 9-37

.. code-block:: default


    # Authors: Michael Sander, Pierre Ablin
    # License: MIT

    import matplotlib.pyplot as plt
    import torch
    from torch import nn
    import numpy as np
    import torch.optim as optim

    from momentumnet import MomentumNet
    from sklearn.datasets import load_digits
    from sklearn.model_selection import train_test_split

    torch.manual_seed(1)
    np.random.seed(1)

    X, y = load_digits(return_X_y=True)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, shuffle=True
    )

    X_train = torch.tensor(X_train).float()
    X_test = torch.tensor(X_test).float()
    y_train = torch.tensor(y_train)
    y_test = torch.tensor(y_test)








.. GENERATED FROM PYTHON SOURCE LINES 38-40

Architecture
#############################

.. GENERATED FROM PYTHON SOURCE LINES 40-62

.. code-block:: default


    hidden = 32
    n_iters = 10
    N = 1000
    d = X.shape[-1]

    functions = [
        nn.Sequential(nn.Linear(d, hidden), nn.Tanh(), nn.Linear(hidden, d))
        for _ in range(n_iters)
    ]

    # Network
    mresnet = MomentumNet(functions, gamma=0.5)

    net = nn.Sequential(mresnet, nn.Linear(64, 10))
    criterion = nn.CrossEntropyLoss()

    n_epochs = 75
    lr_list = np.ones(n_epochs) * 0.01

    optimizer = optim.Adam(mresnet.parameters(), lr=lr_list[0])








.. GENERATED FROM PYTHON SOURCE LINES 63-65

Training
##################################

.. GENERATED FROM PYTHON SOURCE LINES 65-84

.. code-block:: default


    test_error_mresnet = []
    for i in range(n_epochs):
        for param_group in optimizer.param_groups:
            param_group["lr"] = lr_list[i]
        optimizer.zero_grad()
        output = mresnet(X_train)
        loss = criterion(output, y_train)
        loss.backward()
        optimizer.step()
        if i % 30 == 0:
            print("itr %s, loss = %.3f" % (i, loss.item()))
            print("- " * 20)
        _, pred = mresnet(X_test).max(1)
        test_error_mresnet.append(
            (1 - pred.eq(y_test).sum().item() / y_test.shape[0]) * 100
        )






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    itr 0, loss = 13.155
    - - - - - - - - - - - - - - - - - - - - 
    itr 30, loss = 0.045
    - - - - - - - - - - - - - - - - - - - - 
    itr 60, loss = 0.004
    - - - - - - - - - - - - - - - - - - - - 




.. GENERATED FROM PYTHON SOURCE LINES 85-87

Same for a ResNet
########################

.. GENERATED FROM PYTHON SOURCE LINES 87-117

.. code-block:: default


    functions = [
        nn.Sequential(nn.Linear(d, hidden), nn.Tanh(), nn.Linear(hidden, d))
        for _ in range(n_iters)
    ]

    resnet = MomentumNet(functions, gamma=0.0)

    net = nn.Sequential(resnet, nn.Linear(64, 10))
    criterion = nn.CrossEntropyLoss()

    optimizer = optim.Adam(resnet.parameters(), lr=lr_list[0])

    test_error_resnet = []
    for i in range(n_epochs):
        for param_group in optimizer.param_groups:
            param_group["lr"] = lr_list[i]
        optimizer.zero_grad()
        output = resnet(X_train)
        loss = criterion(output, y_train)
        loss.backward()
        optimizer.step()
        if i % 30 == 0:
            print("itr %s, loss = %.3f" % (i, loss.item()))
            print("- " * 20)
        _, pred = resnet(X_test).max(1)
        test_error_resnet.append(
            (1 - pred.eq(y_test).sum().item() / y_test.shape[0]) * 100
        )





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    itr 0, loss = 13.535
    - - - - - - - - - - - - - - - - - - - - 
    itr 30, loss = 0.050
    - - - - - - - - - - - - - - - - - - - - 
    itr 60, loss = 0.004
    - - - - - - - - - - - - - - - - - - - - 




.. GENERATED FROM PYTHON SOURCE LINES 118-120

Plotting the learning curves
####################################

.. GENERATED FROM PYTHON SOURCE LINES 120-128

.. code-block:: default


    plt.figure(figsize=(8, 4))
    plt.semilogy(test_error_mresnet, label="Momentum ResNet", color="red", lw=2.5)
    plt.semilogy(test_error_resnet, label="ResNet", color="darkblue", lw=2.5)
    plt.xlabel("Epochs")
    plt.ylabel("Test error")
    plt.legend()
    plt.show()



.. image:: /auto_examples/images/sphx_glr_plot_learning_digits_001.png
    :alt: plot learning digits
    :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 1 minutes  25.418 seconds)


.. _sphx_glr_download_auto_examples_plot_learning_digits.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_learning_digits.py <plot_learning_digits.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_learning_digits.ipynb <plot_learning_digits.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
